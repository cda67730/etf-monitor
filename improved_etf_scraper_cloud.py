# improved_etf_scraper_cloud_final.py - å®Œå…¨ä¿®å¾©ç‰ˆæœ¬ï¼ˆè§£æ±ºæ‰€æœ‰é‚è¼¯å•é¡Œï¼‰
import requests
import time
import json
from datetime import datetime, timedelta
import logging
import traceback
import re
from database_config import db_config

# Cloud Run å‹å–„çš„æ—¥èªŒè¨­å®š
logging.basicConfig(
    level=logging.INFO, 
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler()  # è¼¸å‡ºåˆ° stdoutï¼ŒCloud Run æœƒè‡ªå‹•æ”¶é›†
    ]
)
logger = logging.getLogger(__name__)

class ETFHoldingsScraper:
    def __init__(self):
        self.base_url = 'https://www.pocket.tw/api/cm/MobileService/ashx/GetDtnoData.ashx'
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Referer': 'https://www.pocket.tw/etf/tw/',
            'Accept': 'application/json, text/javascript, */*; q=0.01',
            'Accept-Language': 'zh-TW,zh;q=0.9,en;q=0.8',
            'X-Requested-With': 'XMLHttpRequest'
        }
        
        # çµ±ä¸€çš„DtNoï¼Œæ‰€æœ‰ETFéƒ½ä½¿ç”¨ç›¸åŒçš„
        self.dtno = '59449513'
        
        # æ”¯æ´çš„ETFä»£ç¢¼æ¸…å–®
        self.etf_codes = ['00980A', '00981A', '00982A', '00983A', '00984A', '00985A']
        
        # ğŸ”§ é—œéµä¿®å¾©ï¼šé©—è­‰æ•¸æ“šåº«å¯ç”¨æ€§
        if not db_config:
            raise Exception("âŒ æ•¸æ“šåº«é…ç½®ä¸å¯ç”¨ï¼Œç„¡æ³•åˆå§‹åŒ–çˆ¬èŸ²")
        
        logger.info(f"âœ… åˆå§‹åŒ–çˆ¬èŸ²ï¼Œæ•¸æ“šåº«é¡å‹: {db_config.db_type}")
        self.init_database()
    
    def parse_date_from_api(self, date_str):
        """ğŸ”§ é—œéµä¿®å¾©ï¼šè§£æAPIè¿”å›çš„æ—¥æœŸ"""
        try:
            if not date_str or not isinstance(date_str, str):
                logger.warning(f"âš ï¸ ç„¡æ•ˆçš„æ—¥æœŸå­—ç¬¦ä¸²: {date_str}")
                return datetime.now().strftime('%Y-%m-%d')
            
            date_str = date_str.strip()
            
            # å¸¸è¦‹çš„æ—¥æœŸæ ¼å¼
            date_patterns = [
                r'(\d{4})[/-](\d{1,2})[/-](\d{1,2})',  # YYYY-MM-DD æˆ– YYYY/MM/DD
                r'(\d{1,2})[/-](\d{1,2})[/-](\d{4})',  # MM/DD/YYYY æˆ– DD/MM/YYYY
                r'(\d{4})(\d{2})(\d{2})',              # YYYYMMDD
            ]
            
            for pattern in date_patterns:
                match = re.search(pattern, date_str)
                if match:
                    groups = match.groups()
                    if len(groups) == 3:
                        # åˆ¤æ–·å¹´ä»½ä½ç½®
                        if len(groups[0]) == 4:  # YYYY-MM-DD
                            year, month, day = groups
                        elif len(groups[2]) == 4:  # MM/DD/YYYY
                            month, day, year = groups
                        else:
                            continue
                        
                        try:
                            parsed_date = datetime(int(year), int(month), int(day))
                            result = parsed_date.strftime('%Y-%m-%d')
                            logger.debug(f"ğŸ“… è§£ææ—¥æœŸ: '{date_str}' â†’ '{result}'")
                            return result
                        except ValueError:
                            continue
            
            # å¦‚æœéƒ½è§£æå¤±æ•—ï¼Œä½¿ç”¨ç•¶å‰æ—¥æœŸ
            logger.warning(f"âš ï¸ ç„¡æ³•è§£ææ—¥æœŸæ ¼å¼: '{date_str}'ï¼Œä½¿ç”¨ç•¶å‰æ—¥æœŸ")
            return datetime.now().strftime('%Y-%m-%d')
            
        except Exception as e:
            logger.error(f"âŒ æ—¥æœŸè§£æéŒ¯èª¤: {e}")
            return datetime.now().strftime('%Y-%m-%d')
    
    def init_database(self):
        """åˆå§‹åŒ–æ•¸æ“šåº«è¡¨"""
        try:
            # æ ¹æ“šæ•¸æ“šåº«é¡å‹èª¿æ•´SQLèªæ³•
            if db_config.db_type == "postgresql":
                id_type = "SERIAL PRIMARY KEY"
                timestamp_default = "TIMESTAMP DEFAULT CURRENT_TIMESTAMP"
            else:
                id_type = "INTEGER PRIMARY KEY AUTOINCREMENT"
                timestamp_default = "TIMESTAMP DEFAULT CURRENT_TIMESTAMP"
            
            # ä¸»è¦æŒè‚¡è¡¨
            holdings_table_sql = f'''
                CREATE TABLE IF NOT EXISTS etf_holdings (
                    id {id_type},
                    etf_code TEXT NOT NULL,
                    stock_code TEXT NOT NULL,
                    stock_name TEXT NOT NULL,
                    weight REAL NOT NULL,
                    shares INTEGER NOT NULL,
                    unit TEXT DEFAULT 'è‚¡',
                    update_date TEXT NOT NULL,
                    created_at {timestamp_default}
                )
            '''
            
            # æŒè‚¡è®ŠåŒ–è¡¨
            changes_table_sql = f'''
                CREATE TABLE IF NOT EXISTS holdings_changes (
                    id {id_type},
                    etf_code TEXT NOT NULL,
                    stock_code TEXT NOT NULL,
                    stock_name TEXT NOT NULL,
                    change_type TEXT NOT NULL,
                    old_shares INTEGER DEFAULT 0,
                    new_shares INTEGER DEFAULT 0,
                    old_weight REAL DEFAULT 0.0,
                    new_weight REAL DEFAULT 0.0,
                    change_date TEXT NOT NULL,
                    created_at {timestamp_default}
                )
            '''
            
            db_config.execute_query(holdings_table_sql)
            db_config.execute_query(changes_table_sql)
            
            # å‰µå»ºç´¢å¼•
            index_sqls = [
                'CREATE INDEX IF NOT EXISTS idx_etf_date ON etf_holdings(etf_code, update_date)',
                'CREATE INDEX IF NOT EXISTS idx_changes_date ON holdings_changes(etf_code, change_date)'
            ]
            
            if db_config.db_type == "postgresql":
                # PostgreSQL ç‰¹æœ‰çš„å”¯ä¸€ç´¢å¼•
                try:
                    unique_index_sql = 'CREATE UNIQUE INDEX IF NOT EXISTS idx_unique_holding ON etf_holdings(etf_code, stock_code, update_date)'
                    db_config.execute_query(unique_index_sql)
                except Exception as e:
                    logger.warning(f"å‰µå»ºå”¯ä¸€ç´¢å¼•å¤±æ•—ï¼ˆå¯èƒ½å·²å­˜åœ¨ï¼‰: {e}")
            
            for sql in index_sqls:
                try:
                    db_config.execute_query(sql)
                except Exception as e:
                    logger.warning(f"å‰µå»ºç´¢å¼•å¤±æ•—ï¼ˆå¯èƒ½å·²å­˜åœ¨ï¼‰: {e}")
            
            logger.info(f"âœ… æ•¸æ“šåº«åˆå§‹åŒ–å®Œæˆ - ä½¿ç”¨ {db_config.db_type}")
            
        except Exception as e:
            logger.error(f"âŒ æ•¸æ“šåº«åˆå§‹åŒ–éŒ¯èª¤: {e}")
            raise e
    
    def check_existing_data(self, etf_code, date):
        """ğŸ”§ æ–°å¢ï¼šæª¢æŸ¥æ˜¯å¦å·²æœ‰è©²æ—¥æœŸçš„æ•¸æ“š"""
        try:
            if db_config.db_type == "postgresql":
                placeholder = "%s"
            else:
                placeholder = "?"
            
            query = f'SELECT COUNT(*) as count FROM etf_holdings WHERE etf_code = {placeholder} AND update_date = {placeholder}'
            result = db_config.execute_query(query, (etf_code, date), fetch="one")
            
            count = result['count'] if result else 0
            return count > 0
            
        except Exception as e:
            logger.error(f"âŒ æª¢æŸ¥ç¾æœ‰æ•¸æ“šæ™‚å‡ºéŒ¯: {e}")
            return False
    
    def get_holdings_data(self, etf_code):
        """ç²å–æŒ‡å®šETFçš„æŒè‚¡æ˜ç´°"""
        if etf_code not in self.etf_codes:
            logger.error(f"âŒ ä¸æ”¯æŒçš„ETFä»£ç¢¼: {etf_code}")
            return None
        
        params = {
            'action': 'getdtnodata',
            'DtNo': self.dtno,
            'ParamStr': f'AssignID={etf_code};MTPeriod=0;DTMode=0;DTRange=1;DTOrder=1;MajorTable=M722;',
            'FilterNo': '0'
        }
        
        try:
            logger.info(f"ğŸ”„ é–‹å§‹ç²å– {etf_code} æ•¸æ“š...")
            response = requests.get(self.base_url, params=params, headers=self.headers, timeout=30)
            response.raise_for_status()
            
            data = response.json()
            logger.info(f"âœ… æˆåŠŸç²å– {etf_code} çš„æŒè‚¡æ•¸æ“š")
            return data
            
        except requests.exceptions.Timeout:
            logger.error(f"âŒ ç²å– {etf_code} æ•¸æ“šè¶…æ™‚")
            return None
        except requests.exceptions.RequestException as e:
            logger.error(f"âŒ ç²å– {etf_code} æ•¸æ“šæ™‚ç¶²è·¯éŒ¯èª¤: {e}")
            return None
        except json.JSONDecodeError as e:
            logger.error(f"âŒ è§£æ {etf_code} JSONæ•¸æ“šæ™‚å‡ºéŒ¯: {e}")
            return None
        except Exception as e:
            logger.error(f"âŒ ç²å– {etf_code} æ•¸æ“šæ™‚æœªçŸ¥éŒ¯èª¤: {e}")
            return None
    
    def parse_holdings_data(self, data, etf_code):
        """ğŸ”§ é—œéµä¿®å¾©ï¼šè§£ææŒè‚¡æ˜ç´°æ•¸æ“šï¼ˆä½¿ç”¨APIè¿”å›çš„æ—¥æœŸï¼‰"""
        holdings = []
        
        try:
            # æª¢æŸ¥æ•¸æ“šçµæ§‹
            if not data or 'Data' not in data:
                logger.warning(f"âš ï¸ {etf_code} å›æ‡‰ä¸­æ²’æœ‰ Data æ•¸æ“š")
                return holdings
            
            title = data.get('Title', [])
            rows_data = data.get('Data', [])
            
            logger.info(f"ğŸ“Š {etf_code} æ¬„ä½åç¨±: {title}")
            logger.info(f"ğŸ“Š {etf_code} æ•¸æ“šè¡Œæ•¸: {len(rows_data)}")
            
            if not rows_data:
                logger.warning(f"âš ï¸ {etf_code} Data ç‚ºç©º")
                return holdings
            
            # ğŸ”§ é—œéµä¿®å¾©ï¼šå¾ç¬¬ä¸€ç­†æ•¸æ“šè§£ææ—¥æœŸ
            data_date = None
            if rows_data and len(rows_data[0]) > 0:
                first_date_str = str(rows_data[0][0]).strip()
                data_date = self.parse_date_from_api(first_date_str)
                logger.info(f"ğŸ“… {etf_code} æ•¸æ“šæ—¥æœŸ: {data_date}")
            
            if not data_date:
                data_date = datetime.now().strftime('%Y-%m-%d')
                logger.warning(f"âš ï¸ {etf_code} ç„¡æ³•è§£ææ•¸æ“šæ—¥æœŸï¼Œä½¿ç”¨ç•¶å‰æ—¥æœŸ: {data_date}")
            
            # ğŸ”§ é—œéµä¿®å¾©ï¼šæª¢æŸ¥æ˜¯å¦å·²æœ‰è©²æ—¥æœŸçš„æ•¸æ“š
            if self.check_existing_data(etf_code, data_date):
                logger.info(f"â„¹ï¸ {etf_code} {data_date} çš„æ•¸æ“šå·²å­˜åœ¨ï¼Œå°‡è¦†è“‹")
            
            successful_parsed = 0
            
            # ç¢ºèªæ¬„ä½å°æ‡‰ ["æ—¥æœŸ","æ¨™çš„ä»£è™Ÿ","æ¨™çš„åç¨±","æ¬Šé‡(%)","æŒæœ‰æ•¸","å–®ä½"]
            for i, row in enumerate(rows_data):
                try:
                    if len(row) < 6:
                        logger.warning(f"âš ï¸ ç¬¬{i+1}è¡Œæ•¸æ“šé•·åº¦ä¸è¶³: {row}")
                        continue
                    
                    date_str = str(row[0]).strip()
                    stock_code = str(row[1]).strip()
                    stock_name = str(row[2]).strip()
                    weight_str = str(row[3]).strip()
                    shares_str = str(row[4]).strip()
                    unit = str(row[5]).strip()
                    
                    # è·³éç©ºç™½æˆ–ç„¡æ•ˆè¨˜éŒ„
                    if not stock_code or not stock_name:
                        continue
                    
                    # è™•ç†æ¬Šé‡ - ç›´æ¥è½‰æ›ç‚ºæµ®é»æ•¸
                    try:
                        weight = float(weight_str) if weight_str else 0.0
                    except ValueError:
                        logger.warning(f"âš ï¸ ç„¡æ³•è§£ææ¬Šé‡: {weight_str}")
                        weight = 0.0
                    
                    # è™•ç†æŒæœ‰æ•¸ - ç§»é™¤é€—è™Ÿä¸¦è½‰æ›ç‚ºæ•´æ•¸
                    try:
                        shares = int(shares_str.replace(',', '')) if shares_str else 0
                    except ValueError:
                        logger.warning(f"âš ï¸ ç„¡æ³•è§£ææŒæœ‰æ•¸: {shares_str}")
                        shares = 0
                    
                    holding = {
                        'etf_code': etf_code,
                        'stock_code': stock_code,
                        'stock_name': stock_name,
                        'weight': weight,
                        'shares': shares,
                        'unit': unit,
                        'update_date': data_date  # ğŸ”§ é—œéµä¿®å¾©ï¼šä½¿ç”¨è§£æçš„æ—¥æœŸ
                    }
                    
                    holdings.append(holding)
                    successful_parsed += 1
                        
                except Exception as e:
                    logger.warning(f"âš ï¸ è§£æç¬¬{i+1}è¡Œæ•¸æ“šæ™‚å‡ºéŒ¯: {e}, æ•¸æ“š: {row}")
                    continue
            
            logger.info(f"âœ… {etf_code} è§£æå®Œæˆ: ç¸½è¨ˆ {len(rows_data)} è¡Œï¼ŒæˆåŠŸè§£æ {successful_parsed} ç­†ï¼Œæ—¥æœŸ: {data_date}")
            return holdings
            
        except Exception as e:
            logger.error(f"âŒ è§£æ {etf_code} æŒè‚¡æ•¸æ“šæ™‚å‡ºéŒ¯: {e}")
            logger.error(traceback.format_exc())
            return []
    
    def get_previous_holdings(self, etf_code, current_date):
        """ğŸ”§ ä¿®å¾©ï¼šç²å–å‰ä¸€äº¤æ˜“æ—¥çš„æŒè‚¡æ•¸æ“š"""
        try:
            logger.info(f"ğŸ“… æŸ¥æ‰¾ {etf_code} åœ¨ {current_date} ä¹‹å‰çš„æŒè‚¡æ•¸æ“š...")
            
            # æ ¹æ“šæ•¸æ“šåº«é¡å‹ä½¿ç”¨æ­£ç¢ºçš„åƒæ•¸ä½”ä½ç¬¦
            if db_config.db_type == "postgresql":
                placeholder = "%s"
            else:
                placeholder = "?"
            
            # ç¬¬ä¸€æ­¥ï¼šæ‰¾åˆ°å‰ä¸€å€‹äº¤æ˜“æ—¥
            find_previous_date_query = f'''
                SELECT MAX(update_date) as prev_date
                FROM etf_holdings 
                WHERE etf_code = {placeholder} AND update_date < {placeholder}
            '''
            
            result = db_config.execute_query(
                find_previous_date_query, 
                (etf_code, current_date), 
                fetch="one"
            )
            
            if not result or not result.get('prev_date'):
                logger.info(f"ğŸ“… {etf_code} æ²’æœ‰æ‰¾åˆ°å‰ä¸€äº¤æ˜“æ—¥çš„æ•¸æ“š")
                return {}
            
            previous_date = result['prev_date']
            logger.info(f"ğŸ“… {etf_code} æ‰¾åˆ°å‰ä¸€äº¤æ˜“æ—¥: {previous_date}")
            
            # ç¬¬äºŒæ­¥ï¼šç²å–é‚£ä¸€å¤©çš„è¨˜éŒ„
            get_holdings_query = f'''
                SELECT stock_code, stock_name, weight, shares
                FROM etf_holdings 
                WHERE etf_code = {placeholder} AND update_date = {placeholder}
            '''
            
            holdings_results = db_config.execute_query(
                get_holdings_query, 
                (etf_code, previous_date), 
                fetch="all"
            )
            
            # ç¬¬ä¸‰æ­¥ï¼šå»ºç«‹å­—å…¸
            previous_data = {}
            if holdings_results:
                for row in holdings_results:
                    stock_code = row['stock_code']
                    previous_data[stock_code] = {
                        'stock_name': row['stock_name'],
                        'weight': row['weight'],
                        'shares': row['shares']
                    }
            
            logger.info(f"ğŸ“Š {etf_code} å‰ä¸€æ—¥æŒè‚¡æ•¸é‡: {len(previous_data)}")
            return previous_data
            
        except Exception as e:
            logger.error(f"âŒ ç²å– {etf_code} å‰ä¸€æ—¥æŒè‚¡æ™‚å‡ºéŒ¯: {e}")
            logger.error(traceback.format_exc())
            return {}

    def analyze_holdings_changes(self, etf_code, current_holdings, current_date):
        """ğŸ”§ ä¿®å¾©ï¼šåˆ†ææŒè‚¡è®ŠåŒ–"""
        try:
            logger.info(f"ğŸ” åˆ†æ {etf_code} æŒè‚¡è®ŠåŒ–...")
            
            previous_holdings = self.get_previous_holdings(etf_code, current_date)
            changes = []
            
            # ç•¶å‰æŒè‚¡å­—å…¸
            current_dict = {h['stock_code']: h for h in current_holdings}
            
            # çµ±è¨ˆè®ŠåŒ–
            new_count = 0
            increased_count = 0
            decreased_count = 0
            removed_count = 0
            
            # æª¢æŸ¥æ–°å¢å’Œè®ŠåŒ–çš„è‚¡ç¥¨
            for stock_code, current_data in current_dict.items():
                if stock_code not in previous_holdings:
                    # æ–°å¢çš„è‚¡ç¥¨
                    changes.append({
                        'etf_code': etf_code,
                        'stock_code': stock_code,
                        'stock_name': current_data['stock_name'],
                        'change_type': 'NEW',
                        'old_shares': 0,
                        'new_shares': current_data['shares'],
                        'old_weight': 0.0,
                        'new_weight': current_data['weight'],
                        'change_date': current_date
                    })
                    new_count += 1
                    logger.debug(f"  â• æ–°å¢: {stock_code} ({current_data['stock_name']})")
                else:
                    # æª¢æŸ¥æŒè‚¡æ•¸é‡è®ŠåŒ–
                    old_data = previous_holdings[stock_code]
                    if current_data['shares'] > old_data['shares']:
                        changes.append({
                            'etf_code': etf_code,
                            'stock_code': stock_code,
                            'stock_name': current_data['stock_name'],
                            'change_type': 'INCREASED',
                            'old_shares': old_data['shares'],
                            'new_shares': current_data['shares'],
                            'old_weight': old_data['weight'],
                            'new_weight': current_data['weight'],
                            'change_date': current_date
                        })
                        increased_count += 1
                        logger.debug(f"  ğŸ“ˆ å¢æŒ: {stock_code} {old_data['shares']:,} â†’ {current_data['shares']:,}")
                    elif current_data['shares'] < old_data['shares']:
                        changes.append({
                            'etf_code': etf_code,
                            'stock_code': stock_code,
                            'stock_name': current_data['stock_name'],
                            'change_type': 'DECREASED',
                            'old_shares': old_data['shares'],
                            'new_shares': current_data['shares'],
                            'old_weight': old_data['weight'],
                            'new_weight': current_data['weight'],
                            'change_date': current_date
                        })
                        decreased_count += 1
                        logger.debug(f"  ğŸ“‰ æ¸›æŒ: {stock_code} {old_data['shares']:,} â†’ {current_data['shares']:,}")
            
            # æª¢æŸ¥ç§»é™¤çš„è‚¡ç¥¨
            for stock_code, old_data in previous_holdings.items():
                if stock_code not in current_dict:
                    changes.append({
                        'etf_code': etf_code,
                        'stock_code': stock_code,
                        'stock_name': old_data['stock_name'],
                        'change_type': 'REMOVED',
                        'old_shares': old_data['shares'],
                        'new_shares': 0,
                        'old_weight': old_data['weight'],
                        'new_weight': 0.0,
                        'change_date': current_date
                    })
                    removed_count += 1
                    logger.debug(f"  â– ç§»é™¤: {stock_code} ({old_data['stock_name']})")
            
            logger.info(f"ğŸ“Š {etf_code} è®ŠåŒ–çµ±è¨ˆ: æ–°å¢{new_count}, å¢æŒ{increased_count}, æ¸›æŒ{decreased_count}, ç§»é™¤{removed_count}")
            return changes
            
        except Exception as e:
            logger.error(f"âŒ åˆ†æ {etf_code} æŒè‚¡è®ŠåŒ–æ™‚å‡ºéŒ¯: {e}")
            logger.error(traceback.format_exc())
            return []
    
    def save_to_database(self, holdings, changes=None):
        """ğŸ”§ å®Œå…¨ä¿®å¾©ï¼šå°‡æŒè‚¡æ˜ç´°å’Œè®ŠåŒ–å­˜å…¥è³‡æ–™åº«ï¼ˆæ­£ç¢ºçš„äº‹å‹™è™•ç†ï¼‰"""
        if not holdings:
            logger.warning("âš ï¸ æ²’æœ‰æŒè‚¡æ•¸æ“šéœ€è¦ä¿å­˜")
            return False
        
        etf_code = holdings[0]['etf_code']
        date = holdings[0]['update_date']
        
        logger.info(f"ğŸ’¾ é–‹å§‹ä¿å­˜ {etf_code} çš„ {len(holdings)} ç­†æŒè‚¡æ•¸æ“šåˆ°è³‡æ–™åº« (æ—¥æœŸ: {date})...")
        
        # æ ¹æ“šæ•¸æ“šåº«é¡å‹ä½¿ç”¨æ­£ç¢ºçš„åƒæ•¸ä½”ä½ç¬¦
        if db_config.db_type == "postgresql":
            ph = "%s"
        else:
            ph = "?"
        
        try:
            # ğŸ”§ é—œéµä¿®å¾©ï¼šæ­£ç¢ºçš„äº‹å‹™è™•ç†
            with db_config.get_connection() as conn:
                # ğŸ”§ é—œéµä¿®å¾©ï¼šé—œé–‰autocommitä»¥å•Ÿç”¨äº‹å‹™
                original_autocommit = getattr(conn, 'autocommit', None)
                if original_autocommit is not None:
                    conn.autocommit = False
                
                cursor = conn.cursor()
                
                try:
                    # 1. åˆªé™¤ç•¶æ—¥èˆŠæ•¸æ“š
                    delete_holdings_query = f'DELETE FROM etf_holdings WHERE etf_code = {ph} AND update_date = {ph}'
                    cursor.execute(delete_holdings_query, (etf_code, date))
                    logger.debug(f"ğŸ—‘ï¸ å·²æ¸…ç† {etf_code} {date} çš„èˆŠæŒè‚¡æ•¸æ“š")
                    
                    delete_changes_query = f'DELETE FROM holdings_changes WHERE etf_code = {ph} AND change_date = {ph}'
                    cursor.execute(delete_changes_query, (etf_code, date))
                    logger.debug(f"ğŸ—‘ï¸ å·²æ¸…ç† {etf_code} {date} çš„èˆŠè®ŠåŒ–æ•¸æ“š")
                    
                    # 2. æ’å…¥æŒè‚¡æ•¸æ“š
                    insert_holding_query = f'''
                        INSERT INTO etf_holdings 
                        (etf_code, stock_code, stock_name, weight, shares, unit, update_date)
                        VALUES ({ph}, {ph}, {ph}, {ph}, {ph}, {ph}, {ph})
                    '''
                    
                    holdings_inserted = 0
                    for holding in holdings:
                        cursor.execute(insert_holding_query, (
                            holding['etf_code'], holding['stock_code'], holding['stock_name'],
                            holding['weight'], holding['shares'], holding['unit'], holding['update_date']
                        ))
                        holdings_inserted += 1
                    
                    logger.info(f"âœ… æˆåŠŸæ’å…¥ {etf_code} {holdings_inserted} ç­†æŒè‚¡æ•¸æ“š")
                    
                    # 3. æ’å…¥è®ŠåŒ–æ•¸æ“š
                    changes_inserted = 0
                    if changes:
                        insert_change_query = f'''
                            INSERT INTO holdings_changes 
                            (etf_code, stock_code, stock_name, change_type, old_shares, new_shares, 
                             old_weight, new_weight, change_date)
                            VALUES ({ph}, {ph}, {ph}, {ph}, {ph}, {ph}, {ph}, {ph}, {ph})
                        '''
                        
                        for change in changes:
                            cursor.execute(insert_change_query, (
                                change['etf_code'], change['stock_code'], change['stock_name'],
                                change['change_type'], change['old_shares'], change['new_shares'],
                                change['old_weight'], change['new_weight'], change['change_date']
                            ))
                            changes_inserted += 1
                        
                        logger.info(f"âœ… æˆåŠŸæ’å…¥ {etf_code} {changes_inserted} ç­†è®ŠåŒ–æ•¸æ“š")
                    
                    # 4. æäº¤äº‹å‹™
                    conn.commit()
                    
                    logger.info(f"ğŸ‰ {etf_code} æ•¸æ“šä¿å­˜å®Œæˆï¼Œäº‹å‹™å·²æäº¤")
                    return True
                    
                except Exception as e:
                    # å›æ»¾äº‹å‹™
                    conn.rollback()
                    logger.error(f"âŒ ä¿å­˜ {etf_code} æ•¸æ“šæ™‚å‡ºéŒ¯ï¼Œäº‹å‹™å·²å›æ»¾: {e}")
                    raise e
                
                finally:
                    # ğŸ”§ é—œéµä¿®å¾©ï¼šæ¢å¾©åŸå§‹autocommitè¨­ç½®
                    if original_autocommit is not None:
                        conn.autocommit = original_autocommit
                    
        except Exception as e:
            logger.error(f"âŒ å­˜å…¥è³‡æ–™åº«æ™‚å‡ºéŒ¯: {e}")
            logger.error(f"éŒ¯èª¤è©³æƒ…: {traceback.format_exc()}")
            return False
    
    def scrape_single_etf(self, etf_code):
        """ğŸ”§ ä¿®å¾©ï¼šçˆ¬å–å–®å€‹ETFçš„æŒè‚¡æ˜ç´°"""
        logger.info(f"ğŸ¯ é–‹å§‹è™•ç†: {etf_code}")
        
        try:
            # 1. ç²å–æ•¸æ“š
            data = self.get_holdings_data(etf_code)
            if not data:
                logger.error(f"âŒ ç„¡æ³•ç²å– {etf_code} çš„æ•¸æ“š")
                return False
            
            # 2. è§£ææ•¸æ“š
            holdings = self.parse_holdings_data(data, etf_code)
            if not holdings:
                logger.warning(f"âš ï¸ {etf_code} ç„¡æŒè‚¡æ•¸æ“š")
                return False
            
            # 3. åˆ†æè®ŠåŒ–ï¼ˆä½¿ç”¨è§£æå‡ºçš„æ—¥æœŸï¼‰
            data_date = holdings[0]['update_date']
            changes = self.analyze_holdings_changes(etf_code, holdings, data_date)
            
            # 4. å­˜å…¥è³‡æ–™åº«
            success = self.save_to_database(holdings, changes)
            
            if success:
                logger.info(f"âœ… {etf_code} è™•ç†å®Œæˆ: {len(holdings)} ç­†æŒè‚¡, {len(changes)} é …è®ŠåŒ– (æ—¥æœŸ: {data_date})")
                return True
            else:
                logger.error(f"âŒ {etf_code} æ•¸æ“šä¿å­˜å¤±æ•—")
                return False
            
        except requests.exceptions.RequestException as e:
            logger.error(f"âŒ {etf_code} ç¶²è·¯è«‹æ±‚éŒ¯èª¤: {e}")
            return False
        except Exception as e:
            logger.error(f"âŒ è™•ç† {etf_code} æ™‚ç™¼ç”ŸæœªçŸ¥éŒ¯èª¤: {e}")
            logger.error(traceback.format_exc())
            return False
    
    def scrape_all_etfs(self):
        """ğŸ”§ ä¿®å¾©ï¼šçˆ¬å–æ‰€æœ‰ETFçš„æŒè‚¡æ˜ç´°"""
        start_time = datetime.now()
        logger.info("ğŸš€ é–‹å§‹çˆ¬å–æ‰€æœ‰ETFæŒè‚¡æ˜ç´°")
        logger.info(f"ğŸ“‹ å¾…è™•ç†ETF: {', '.join(self.etf_codes)}")
        logger.info(f"ğŸ• é–‹å§‹æ™‚é–“: {start_time.strftime('%Y-%m-%d %H:%M:%S')}")
        
        success_count = 0
        failed_etfs = []
        
        for i, etf_code in enumerate(self.etf_codes, 1):
            try:
                logger.info(f"\n{'='*60}")
                logger.info(f"ğŸ”„ è™•ç† {etf_code} ({i}/{len(self.etf_codes)})")
                
                if self.scrape_single_etf(etf_code):
                    success_count += 1
                    logger.info(f"âœ… {etf_code} æˆåŠŸ")
                else:
                    failed_etfs.append(etf_code)
                    logger.error(f"âŒ {etf_code} å¤±æ•—")
                
                # é¿å…è«‹æ±‚éæ–¼é »ç¹
                if i < len(self.etf_codes):  # ä¸æ˜¯æœ€å¾Œä¸€å€‹
                    logger.debug("â³ ç­‰å¾… 2 ç§’é¿å…é »ç¹è«‹æ±‚...")
                    time.sleep(2)
                    
            except KeyboardInterrupt:
                logger.warning(f"âš ï¸ ç”¨æˆ¶ä¸­æ–·æ“ä½œï¼Œå·²è™•ç† {i-1}/{len(self.etf_codes)}")
                break
            except Exception as e:
                logger.error(f"âŒ è™•ç† {etf_code} æ™‚ç™¼ç”Ÿåš´é‡éŒ¯èª¤: {e}")
                logger.error(traceback.format_exc())
                failed_etfs.append(etf_code)
        
        # ç¸½çµå ±å‘Š
        end_time = datetime.now()
        duration = end_time - start_time
        
        logger.info(f"\n{'='*60}")
        logger.info("ğŸ“Š çˆ¬å–å®Œæˆç¸½çµ")
        logger.info(f"ğŸ• çµæŸæ™‚é–“: {end_time.strftime('%Y-%m-%d %H:%M:%S')}")
        logger.info(f"â±ï¸ è€—æ™‚: {duration}")
        logger.info(f"âœ… æˆåŠŸ: {success_count}/{len(self.etf_codes)}")
        
        if failed_etfs:
            logger.warning(f"âŒ å¤±æ•—çš„ETF: {', '.join(failed_etfs)}")
        else:
            logger.info("ğŸ‰ å…¨éƒ¨æˆåŠŸï¼")
        
        # ğŸ”§ æ–°å¢ï¼šæä¾›æ•¸æ“šåº«ç‹€æ…‹æ‘˜è¦
        try:
            if db_config.db_type == "postgresql":
                ph = "%s"
            else:
                ph = "?"
            
            summary_query = f'''
                SELECT etf_code, update_date, COUNT(*) as holdings_count
                FROM etf_holdings 
                WHERE update_date = (SELECT MAX(update_date) FROM etf_holdings)
                GROUP BY etf_code, update_date
                ORDER BY etf_code
            '''
            
            summary_results = db_config.execute_query(summary_query, fetch="all")
            if summary_results:
                logger.info("ğŸ“ˆ æœ€æ–°æ•¸æ“šæ‘˜è¦:")
                for row in summary_results:
                    logger.info(f"  {row['etf_code']}: {row['holdings_count']} ç­†æŒè‚¡ ({row['update_date']})")
            
        except Exception as e:
            logger.warning(f"âš ï¸ ç„¡æ³•ç²å–æ•¸æ“šæ‘˜è¦: {e}")
        
        return success_count

    def test_single_request(self, etf_code='00981A'):
        """ğŸ§ª æ¸¬è©¦å–®å€‹è«‹æ±‚ï¼Œç”¨æ–¼ç¢ºèªç¨‹å¼æ­£ç¢ºæ€§"""
        logger.info(f"ğŸ§ª é–‹å§‹æ¸¬è©¦ {etf_code}")
        
        try:
            # æ¸¬è©¦æ•¸æ“šç²å–
            data = self.get_holdings_data(etf_code)
            if not data:
                logger.error("âŒ æ¸¬è©¦å¤±æ•—ï¼šç„¡æ³•ç²å–æ•¸æ“š")
                return None
            
            logger.info(f"ğŸ“Š åŸå§‹å›æ‡‰æ•¸æ“šçµæ§‹: {list(data.keys()) if isinstance(data, dict) else type(data)}")
            
            # è©³ç´°æª¢æŸ¥æ•¸æ“šçµæ§‹
            if isinstance(data, dict):
                for key in data.keys():
                    value = data[key]
                    if isinstance(value, list):
                        logger.info(f"ğŸ”‘ éµ '{key}' åŒ…å« {len(value)} å€‹é …ç›®")
                        if value and len(value) > 0:
                            logger.info(f"ğŸ“„ ç¬¬ä¸€å€‹é …ç›®ç¯„ä¾‹: {value[0]}")
                    else:
                        logger.info(f"ğŸ”‘ éµ '{key}': {type(value)} - {str(value)[:100]}...")
            
            # æ¸¬è©¦æ•¸æ“šè§£æ
            holdings = self.parse_holdings_data(data, etf_code)
            logger.info(f"ğŸ“ˆ è§£æå¾ŒæŒè‚¡æ•¸é‡: {len(holdings)}")
            if holdings:
                logger.info(f"ğŸ“„ ç¬¬ä¸€ç­†æŒè‚¡ç¯„ä¾‹: {holdings[0]}")
                data_date = holdings[0]['update_date']
                logger.info(f"ğŸ“… è§£æå‡ºçš„æ•¸æ“šæ—¥æœŸ: {data_date}")
                
                # æ¸¬è©¦è®ŠåŒ–åˆ†æ
                changes = self.analyze_holdings_changes(etf_code, holdings, data_date)
                logger.info(f"ğŸ“Š è®ŠåŒ–åˆ†æçµæœ: {len(changes)} é …è®ŠåŒ–")
                
                # æ¸¬è©¦æ•¸æ“šåº«æ“ä½œï¼ˆä¸å¯¦éš›ä¿å­˜ï¼‰
                logger.info("ğŸ§ª æ¸¬è©¦æ•¸æ“šåº«é€£æ¥...")
                if db_config.db_type == "postgresql":
                    ph = "%s"
                else:
                    ph = "?"
                
                test_query = f"SELECT COUNT(*) as count FROM etf_holdings WHERE etf_code = {ph}"
                result = db_config.execute_query(test_query, (etf_code,), fetch="one")
                existing_count = result['count'] if result else 0
                logger.info(f"ğŸ“Š æ•¸æ“šåº«ä¸­ç¾æœ‰ {etf_code} è¨˜éŒ„: {existing_count}")
                
                # æ¸¬è©¦é‡è¤‡æ•¸æ“šæª¢æŸ¥
                has_existing = self.check_existing_data(etf_code, data_date)
                logger.info(f"ğŸ” {data_date} æ˜¯å¦å·²æœ‰æ•¸æ“š: {has_existing}")
            
            logger.info("âœ… æ¸¬è©¦å®Œæˆ")
            return holdings
                
        except Exception as e:
            logger.error(f"âŒ æ¸¬è©¦éç¨‹ä¸­å‡ºéŒ¯: {e}")
            logger.error(traceback.format_exc())
            return None